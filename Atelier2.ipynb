{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e9b4a-51aa-4f2f-91a3-ad0be6190542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "# Transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Pour VGG, AlexNet, etc.\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Chargement des donn√©es MNIST\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adfa8b4-3d48-4137-b8ad-32b6d0dd62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Partie 1 : CNN Classifier ----------------------------\n",
    "# ********** 1.1 CNN Custom **************\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*56*56, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model_cnn = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f509a0-8e23-4cb7-9506-155c09faf107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** 1.2 Entra√Ænement CNN ************\n",
    "\n",
    "def train_model(model, name):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(5):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"{name} Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    print(f\"{name} Training Time: {time.time() - start:.2f}s\")\n",
    "\n",
    "train_model(model_cnn, \"CNN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7622f44-8f8f-4bc2-9887-fbd7172e39f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** 1.3 √âvaluation CNN ***************\n",
    "\n",
    "def evaluate(model, name):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1).cpu()\n",
    "            y_true.extend(labels)\n",
    "            y_pred.extend(preds)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "evaluate(model_cnn, \"CNN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d4df1-e533-4742-be2b-4702c86e3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********* 3. Fine-tuning VGG16 et AlexNet ************\n",
    "\n",
    "def fine_tune_model(model_name):\n",
    "    model = getattr(models, model_name)(pretrained=True)\n",
    "    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 10)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"{model_name} Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    evaluate(model, model_name)\n",
    "\n",
    "fine_tune_model(\"vgg16\")\n",
    "fine_tune_model(\"alexnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0de9f3-eb95-4726-8660-d04301f81b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------üîπ Partie 2 : Vision Transformer (ViT) from Scratch ---------------------------\n",
    "\n",
    "# *************** 1. Impl√©mentation ViT (extrait du tutoriel Medium) *****************\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.pos_embed = nn.Parameter(torch.randn((img_size // patch_size)**2 + 1, emb_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embed\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size=768, heads=8, dropout=0.1, forward_expansion=4):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=emb_size, num_heads=heads)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size, forward_expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(forward_expansion * emb_size, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.ln1(x + attn_out)\n",
    "        x = self.ln2(x + self.mlp(x))\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, emb_size=768, num_classes=10, depth=6, heads=8):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(in_channels=3, patch_size=patch_size, emb_size=emb_size, img_size=img_size)\n",
    "        self.encoder = nn.Sequential(*[TransformerEncoderBlock(emb_size, heads) for _ in range(depth)])\n",
    "        self.cls_head = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder(x)\n",
    "        cls_token = x[:, 0]\n",
    "        return self.cls_head(cls_token)\n",
    "\n",
    "model_vit = ViT().to(device)\n",
    "optimizer = optim.Adam(model_vit.parameters(), lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c362e-2225-4232-b0bf-0127dd7d42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Entra√Ænement & √âvaluation ViT\n",
    "\n",
    "train_model(model_vit, \"ViT\")\n",
    "evaluate(model_vit, \"ViT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa8f6d-07be-4444-8492-66e935b0a4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958330b-600d-49e7-ac5b-785fa4a7ac1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
